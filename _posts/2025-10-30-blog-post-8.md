---
title: 'Blog 8:  Harms in Machine Learning'
date: 2025-10-30
permalink: /posts/2025/10/blog-post-8/
tags:
  - T2I AI
  - South Asia
  - Society
  - Qualitative Research
---

**Case Study:**  
[Understanding Potential Sources of Harm throughout the Machine Learning Life Cycle](https://mit-serc.pubpub.org/pub/potential-sources-of-harm-throughout-the-machine-learning-life-cycle/release/2)

In today's blog I reflect on perhaps the most critical case study so far. The reason is because it goes over the root of all of the AI systems we see today. Machine learning is at the core of AI develoment and involves a lengthy process of training based on human data. One of the topics the authors of the case study discuss is the sources of harm the Machine Learning pipeline (the process of designing, developing and deploying a machine learning model).

At the beginning of the process, there is always a risk of historical bias in the data we are using to train a ML model. The world as it is is heavily biased and every real-world data comes with those biases embedded in it. One of those biases is about reinforcing certain preexisting stereotypes against a particular group. For example, imagine you want to train a machine learning model that detects the geographical location of a place based on an outdoor picture. The resulting model is more likely to be accurate when it comes to pinpointing locations in the western world because most people here have access to a smartphone to capture high quality pictures. However, the ML model might not be so lucky in localizing pictures in remote mountainous areas in Africa and instead might guess that the picture is in some caribbean islands, some of the most-visited touristic destinations. The reason for this is because the majority of the population in remote areas in Africa do not have access to smartphones that take pictures so the only pictures that the dataset might get are the ones in urban areas which do not resemble the whole Africa. This example highlights how ML models can oversimplify complex geographies simply because the data that it has been fed represent only one group of a certain location.

A second source of bias is from the fact that the training data can underrepresent certain groups of population. Let us take for example an AI poem generator. Because of a lack of poem samples from non-western countries online, the AI might struggle to generate poems resembling non-western genres but will definitely have no problem when asked to generate poems resembling one of Moliere's poems.

A third source of bias is when a label is used to approximate a very complex social situation. As a result of this, there is an oversimplification of the said social construct and the predictor. An example is the use of standardized tests scores to gauge how well a student will do in law school. The problem with standardized tests is that they are very predictable so anybody who spend time preparing for them can do them. On the other hand, law is such an ever-evolving discipline that requires constant education so the one-time test does not really matter in the long run.

A fourth source of bias is when a more general model is used for representing a population that contain different subsets, hence needs a particular consideration. For example, imagine if we were to build a predictor of how well STEM majors do after school based on how many years after graduation they got their first job. Although this model can work for certain STEM majors that can easily get a job after undergraduate studies, there is a whole other world of STEM specializations that require further studies in order to begin working in the industry so the proposed model would be heavily biased against the latter.

A fifth source of bias comes from the learning phase of a model. When it is tuned to focus on a certain metric while missing out on another. For example, if a model is being trained on data that represents MPG and Horsepower of cars, it might assume at first that there is a linear regression of those two variables even though the horsepower can be increased without necessarily touching the fuel system of a car.

A sixth source of bias is a result of the narrowness of evaluation benchmarks that are used to evaluate different AI models. The bias can be tied to the representation bias discussed earlier.